{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5beafc64",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import pandas as pd\n",
    "from pandasql import sqldf\n",
    "import re\n",
    "from langchain.document_loaders.csv_loader import CSVLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.llms import CTransformers\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.chains import ConversationalRetrievalChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ef098789",
   "metadata": {},
   "outputs": [],
   "source": [
    " \n",
    "def load_data(path):\n",
    "    df = pd.read_csv(path)  \n",
    "    return df\n",
    " \n",
    "def calculate_chunk_size(df):\n",
    "    column_names = df.columns\n",
    "    first_row_values = df.iloc[0]\n",
    "    total_chars_in_column_names = sum(len(name) for name in column_names)\n",
    "    total_chars_in_first_row = sum(len(str(value)) for value in first_row_values)\n",
    "    chunk_size = int((total_chars_in_column_names + total_chars_in_first_row)*1.4)\n",
    "    return chunk_size\n",
    " \n",
    "def load_and_split_data(path, chunk_size):\n",
    "    loader = CSVLoader(file_path=path, encoding=\"utf-8\", csv_args={'delimiter': ','})\n",
    "    data = loader.load()\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=0)\n",
    "    text_chunks = text_splitter.split_documents(data)\n",
    "    return text_chunks\n",
    " \n",
    "def create_embeddings(text_chunks, model_name):\n",
    "    embeddings = HuggingFaceEmbeddings(model_name = model_name)\n",
    "    docsearch = FAISS.from_documents(text_chunks, embeddings)\n",
    "    return docsearch\n",
    " \n",
    "def save_embeddings(docsearch, DB_FAISS_PATH):\n",
    "    docsearch.save_local(DB_FAISS_PATH)\n",
    " \n",
    "def load_llm_model(model_path, config):\n",
    "    llm = CTransformers(model=model_path,\n",
    "                        model_type=\"llama\",\n",
    "                        config=config)\n",
    "    return llm\n",
    " \n",
    "def create_conversational_chain(llm, retriever):\n",
    "    qa = ConversationalRetrievalChain.from_llm(llm, retriever=retriever)\n",
    "    return qa\n",
    " \n",
    "def chat(qa):\n",
    "    while(True):\n",
    "        chat_history = []\n",
    "        query = input(f\"Input Prompt: \")\n",
    "        if query == 'exit':\n",
    "            print('Exiting')\n",
    "            sys.exit()\n",
    "        edited_query =  query \n",
    "        result = qa({\"question\": edited_query, \"chat_history\":chat_history})\n",
    "        Response = result['answer']\n",
    "        print(\"Response: \",Response)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "359f91d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Prompt: What is the purpose of the dataset? \n",
      "Response:   The purpose of this dataset is to provide information about passengers on the Titanic, including their name, age, sex, class, fare, and survival status. This information can be used for various purposes such as research, analysis, or visualization.\n",
      "Input Prompt: What type of data is it (e.g., marketing, transactional, dimensional, etc.)? \n",
      "Response:   Based on the provided context, it appears that this data is transactional in nature. The data includes information about passengers on the Titanic, including their names, ages, genders, ticket numbers, fares, and cabin assignments. This type of data is typically used for business purposes, such as managing bookings, tracking passenger information, and conducting market research.\n",
      "Input Prompt: What are the key entities in the dataset? \n",
      "Response:   The key entities in the dataset are Passenger_Id, Survived, Pclass, Name, Sex, Age, SibSp, and Parch. These entities represent the different attributes of each passenger, such as their unique identifier, whether they survived or not, their cabin class, and other personal information.\n",
      "Input Prompt: What are the relationships between the entities? \n",
      "Response:   The relationships between the entities can be represented as a directed graph, where each entity is a node in the graph and the edges represent the relationships between them. For example, Mrs. Turkula is embarked on the ship (B57) and is also a passenger on the ship (484).\n",
      "Input Prompt: Identify the columns that has values as codes and require another description fields to understand the business value.\n",
      "Response:   The columns that have values as codes are 'Pclass' and 'Ticket'. In these columns, the codes represent the different classes of accommodation and fares paid by passengers. Additional description fields such as 'Name', 'Sex', 'Age', 'SibSp', and 'Parch' provide additional information about each passenger, including their identity, gender, age, number of siblings, and whether they were traveling with a parent or spouse.\n",
      "Input Prompt: Identify the columns that have junk values. For Example. Your field name is Country and Delhi is one of the available values. \n",
      "Response:   The Cabin column has a junk value for Passenger_Id = 825, as it contains the string \"S\".\n",
      "Input Prompt: Suggest suitable data types, (int, float, double etc)\n",
      "Response:  \n",
      "For the fare and cabin information, you would likely want to use an int (integer) for the fare and a string (e.g. \"A16\") for the cabin.\n",
      "For the embarked information, you would likely want to use a boolean (true or false) since it can only have two values.\n",
      "For the ticket information, you would likely want to use an int (integer) for the ticket number and a string (e.g. \"108.9\") for the fare.\n",
      "For the passenger information, you would likely want to use an integer (int) for the passenger ID and a string (e.g. \"Mrs. Leopold Weisz\") for the name.\n",
      "You may also want to consider using a date data type (e.g. Date) for the age and sibsp information, since these values are dates.\n",
      "\n",
      "End of Helpful Answer\n",
      "Input Prompt: Identify the date columns, and its format? Identify if different date formats available in single column. \n",
      "Response:   The date columns are identified as 'Embarked' and 'S'. The format of these columns is not explicitly mentioned in the provided context, but based on the information provided, it can be inferred that the dates are represented in the format of 'DD/MM/YYYY'.\n",
      "Input Prompt: Identify Trends. Ex â€“ rise is sales on every holiday season. \n",
      "Response:   The trend in the given data is that there is a steady increase in fare from Cabin A16 and Embarked C, while there is a steady decrease in fare from Cabin E34 and Embarked S.\n",
      "Input Prompt: exit\n",
      "Exiting\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[1;31mSystemExit\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ShubhamGupta\\anaconda3\\Lib\\site-packages\\IPython\\core\\interactiveshell.py:3534: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    path = r\"C:\\Users\\ShubhamGupta\\Downloads\\Data.csv\"\n",
    "    DB_FAISS_PATH = r\"C:\\Users\\ShubhamGupta\\db_faiss\"\n",
    "    model_path = r\"C:\\Users\\ShubhamGupta\\Downloads\\llama-2-7b-chat.ggmlv3.q8_0.bin\"\n",
    "    model_name = 'sentence-transformers/all-MiniLM-L6-v2'\n",
    "    config = {\"max_new_tokens\": 2048, \"context_length\": 4096 , \"temperature\": 0}\n",
    " \n",
    "    df = load_data(path)\n",
    "    chunk_size = calculate_chunk_size(df)\n",
    "    text_chunks = load_and_split_data(path, chunk_size)\n",
    "    docsearch = create_embeddings(text_chunks, model_name)\n",
    "    save_embeddings(docsearch, DB_FAISS_PATH)\n",
    "    llm = load_llm_model(model_path, config)\n",
    "    qa = create_conversational_chain(llm, docsearch.as_retriever())\n",
    "    chat(qa)\n",
    " \n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3200bde6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
